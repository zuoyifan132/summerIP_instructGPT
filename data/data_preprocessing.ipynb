{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e031c833-6144-4389-8f18-84d298b15729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /Users/evan/Library/Python/3.9/lib/python/site-packages (2.13.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: packaging in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pandas in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (2.0.1)\n",
      "Requirement already satisfied: xxhash in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (3.2.0)\n",
      "Requirement already satisfied: multiprocess in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (12.0.1)\n",
      "Requirement already satisfied: aiohttp in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (0.15.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (2.30.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: filelock in /Library/Python/3.9/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.15.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/evan/Library/Python/3.9/lib/python/site-packages (4.30.2)\n",
      "Requirement already satisfied: requests in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: filelock in /Library/Python/3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /Users/evan/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.1.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seqeval in /Users/evan/Library/Python/3.9/lib/python/site-packages (1.2.2)\n",
      "Requirement already satisfied: numpy>=1.14.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from seqeval) (1.24.3)\n",
      "Requirement already satisfied: scikit-learn>=0.21.3 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from seqeval) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.21.3->seqeval) (1.10.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /Users/evan/Library/Python/3.9/lib/python/site-packages (4.30.2)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.3.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: filelock in /Library/Python/3.9/site-packages (from transformers) (3.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (2023.6.3)\n",
      "Requirement already satisfied: requests in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (2.30.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (0.15.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: fsspec in /Users/evan/Library/Python/3.9/lib/python/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (2023.5.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/evan/Library/Python/3.9/lib/python/site-packages (from requests->transformers) (1.26.6)\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 23.1.2 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "zsh:1: no matches found: transformers[torch]\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets\n",
    "!pip3 install transformers \n",
    "!pip3 install seqeval\n",
    "!pip3 install transformers -U\n",
    "!pip3 install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b650d1e5-5e71-4948-b692-2c41c0314697",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "from datasets import load_dataset,concatenate_datasets, load_metric\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ba83959-0096-4e24-87a8-c274ed9492b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f0dcd9b637342a7bc430c6ac1baa55d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed75cd200ad8422f8f35a03f01c44aac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fbb82491d6462297e4a339824cd2b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset wnut_17 (/Users/evan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902f3944e4044562b456bf09892f8930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wnut = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9ec644-a2d0-4cbf-8f9f-6a3541afcf05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'O',\n",
       " 1: 'B-corporation',\n",
       " 2: 'I-corporation',\n",
       " 3: 'B-creative-work',\n",
       " 4: 'I-creative-work',\n",
       " 5: 'B-group',\n",
       " 6: 'I-group',\n",
       " 7: 'B-location',\n",
       " 8: 'I-location',\n",
       " 9: 'B-person',\n",
       " 10: 'I-person',\n",
       " 11: 'B-product',\n",
       " 12: 'I-product'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list = wnut[\"train\"].features[f\"ner_tags\"].feature.names\n",
    "id2tag = {id: tag for id, tag in enumerate(label_list)}\n",
    "id2tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4aec57e9-9615-430f-8ae4-93d9337d357b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags'],\n",
       "    num_rows: 4403\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# merge train & validation sets\n",
    "from datasets import concatenate_datasets\n",
    "\n",
    "train_dataset = concatenate_datasets([wnut[\"train\"],wnut[\"validation\"]])\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ff70ca9-2854-497d-9c38-237657c7a388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Pxleyes', 'Top', '50', 'Photography', 'Contest', 'Pictures', 'of', 'August', '2010', '...', 'http://bit.ly/bgCyZ0', '#photography']\n",
      "[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "['B-corporation', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "ith_example=2\n",
    "\n",
    "print(train_dataset[ith_example]['tokens'])\n",
    "print(train_dataset[ith_example]['ner_tags'])\n",
    "print([id2tag[label] for label in train_dataset[ith_example]['ner_tags']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cff361c2-5d36-479b-8d30-00d849963fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43c94f75-4229-4b0e-9b98-470d912dc123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[f\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:  # Set the special tokens to -100.\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db62d98a-7984-4dc8-9f64-67b10f46fe1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /Users/evan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-2fe5dae7720f7a18.arrow\n",
      "Loading cached processed dataset at /Users/evan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-3c176d959584da3a.arrow\n",
      "Loading cached processed dataset at /Users/evan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/077c7f08b8dbc800692e8c9186cdf3606d5849ab0e7be662e6135bb10eba54f9/cache-3e12ab8b5661768d.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4403 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'tokens', 'ner_tags', 'input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 4403\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_train_dataset = train_dataset.map(tokenize_and_align_labels, batched=True)\n",
    "tokenized_train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f951d6f-1a7d-4e88-b209-6342822d0128",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tokens</th>\n",
       "      <th>ner_tags</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>token_type_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[@paulwalk, It, 's, the, view, from, where, I,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...</td>\n",
       "      <td>[101, 1030, 2703, 17122, 2009, 1005, 1055, 199...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, -100, -100, 0, 0, -100, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[From, Green, Newsfeed, :, AHFA, extends, dead...</td>\n",
       "      <td>[0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2013, 2665, 2739, 7959, 2098, 1024, 6289...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, -100, -100, 0, 5, -100, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[Pxleyes, Top, 50, Photography, Contest, Pictu...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 1052, 20959, 2229, 2327, 2753, 5855, 504...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 1, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2651, 2003, 2026, 2197, 2154, 2012, 1996...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[4Dbling, 's, place, til, monday, ,, party, pa...</td>\n",
       "      <td>[9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 1018, 18939, 2989, 1005, 1055, 2173, 186...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 9, -100, -100, 0, -100, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4398</th>\n",
       "      <td>1004</td>\n",
       "      <td>[your, sarcasm, is, goals, :, D, :, D]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2115, 20954, 2003, 3289, 1024, 1040, 102...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4399</th>\n",
       "      <td>1005</td>\n",
       "      <td>[I, HATE, THIS, I, HATE, THIS, I, HATE, THIS, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 1045, 5223, 2023, 1045, 5223, 2023, 1045...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>1006</td>\n",
       "      <td>[For, education, ., Lol, yeah, like, my, dads,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[101, 2005, 2495, 1012, 8840, 2140, 3398, 2066...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[-100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4401</th>\n",
       "      <td>1007</td>\n",
       "      <td>[excellent, poem, Dubbz﻿]</td>\n",
       "      <td>[0, 0, 9]</td>\n",
       "      <td>[101, 6581, 5961, 12931, 2497, 2480, 102]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 9, -100, -100, -100]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4402</th>\n",
       "      <td>1008</td>\n",
       "      <td>[This, guy, needs, his, own, show, on, Discive...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 2, 0]</td>\n",
       "      <td>[101, 2023, 3124, 3791, 2010, 2219, 2265, 2006...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "      <td>[-100, 0, 0, 0, 0, 0, 0, 0, 1, -100, -100, 2, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4403 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             tokens   \n",
       "0        0  [@paulwalk, It, 's, the, view, from, where, I,...  \\\n",
       "1        1  [From, Green, Newsfeed, :, AHFA, extends, dead...   \n",
       "2        2  [Pxleyes, Top, 50, Photography, Contest, Pictu...   \n",
       "3        3     [today, is, my, last, day, at, the, office, .]   \n",
       "4        4  [4Dbling, 's, place, til, monday, ,, party, pa...   \n",
       "...    ...                                                ...   \n",
       "4398  1004             [your, sarcasm, is, goals, :, D, :, D]   \n",
       "4399  1005  [I, HATE, THIS, I, HATE, THIS, I, HATE, THIS, ...   \n",
       "4400  1006  [For, education, ., Lol, yeah, like, my, dads,...   \n",
       "4401  1007                          [excellent, poem, Dubbz﻿]   \n",
       "4402  1008  [This, guy, needs, his, own, show, on, Discive...   \n",
       "\n",
       "                                               ner_tags   \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, ...  \\\n",
       "1         [0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2                  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3                           [0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4                  [9, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "...                                                 ...   \n",
       "4398                           [0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4399         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4400      [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4401                                          [0, 0, 9]   \n",
       "4402                     [0, 0, 0, 0, 0, 0, 0, 1, 2, 0]   \n",
       "\n",
       "                                              input_ids   \n",
       "0     [101, 1030, 2703, 17122, 2009, 1005, 1055, 199...  \\\n",
       "1     [101, 2013, 2665, 2739, 7959, 2098, 1024, 6289...   \n",
       "2     [101, 1052, 20959, 2229, 2327, 2753, 5855, 504...   \n",
       "3     [101, 2651, 2003, 2026, 2197, 2154, 2012, 1996...   \n",
       "4     [101, 1018, 18939, 2989, 1005, 1055, 2173, 186...   \n",
       "...                                                 ...   \n",
       "4398  [101, 2115, 20954, 2003, 3289, 1024, 1040, 102...   \n",
       "4399  [101, 1045, 5223, 2023, 1045, 5223, 2023, 1045...   \n",
       "4400  [101, 2005, 2495, 1012, 8840, 2140, 3398, 2066...   \n",
       "4401          [101, 6581, 5961, 12931, 2497, 2480, 102]   \n",
       "4402  [101, 2023, 3124, 3791, 2010, 2219, 2265, 2006...   \n",
       "\n",
       "                                         token_type_ids   \n",
       "0     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \\\n",
       "1     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "...                                                 ...   \n",
       "4398                     [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "4399  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4400  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4401                              [0, 0, 0, 0, 0, 0, 0]   \n",
       "4402         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "\n",
       "                                         attention_mask   \n",
       "0     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \\\n",
       "1     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "...                                                 ...   \n",
       "4398                     [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "4399  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4400  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4401                              [1, 1, 1, 1, 1, 1, 1]   \n",
       "4402         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]   \n",
       "\n",
       "                                                 labels  \n",
       "0     [-100, 0, -100, -100, 0, 0, -100, 0, 0, 0, 0, ...  \n",
       "1     [-100, 0, 0, 0, -100, -100, 0, 5, -100, 0, 0, ...  \n",
       "2     [-100, 1, -100, -100, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3               [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100]  \n",
       "4     [-100, 9, -100, -100, 0, -100, 0, 0, 0, 0, 0, ...  \n",
       "...                                                 ...  \n",
       "4398               [-100, 0, 0, 0, 0, 0, 0, 0, 0, -100]  \n",
       "4399  [-100, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -100, 0, ...  \n",
       "4400  [-100, 0, 0, 0, 0, -100, 0, 0, 0, 0, -100, 0, ...  \n",
       "4401                  [-100, 0, 0, 9, -100, -100, -100]  \n",
       "4402  [-100, 0, 0, 0, 0, 0, 0, 0, 1, -100, -100, 2, ...  \n",
       "\n",
       "[4403 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dataset to a Pandas DataFrame\n",
    "train_df = pd.DataFrame(tokenized_train_dataset.to_pandas())\n",
    "\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f3b43b90-c58f-458a-8b13-91800c24a016",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(pred_labels, true_labels):\n",
    "    num_O = sum([1 for i in true_labels if i == 'O'])\n",
    "    num_entity = len(true_labels) - num_O\n",
    "\n",
    "    num_O_correct = sum([1 for (p, l) in zip(pred_labels, true_labels) if p == l and l == 'O'])\n",
    "    num_entity_correct = sum([1 for (p, l) in zip(pred_labels, true_labels) if p == l and l != 'O'])\n",
    "    \n",
    "    O_rate = num_O_correct/num_O if num_O != 0 else 0\n",
    "    entity_rate = num_entity_correct/num_entity if num_entity != 0 else 0\n",
    "\n",
    "    if num_O == 0:\n",
    "        reward = entity_rate * 10\n",
    "        print(2)\n",
    "        return reward\n",
    "    if num_entity == 0:\n",
    "        reward = O_rate * 10\n",
    "        print(1)\n",
    "        return reward\n",
    "        \n",
    "    reward = O_rate * 4 + entity_rate * 6\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dbb79306-1ef5-45a5-9cd8-26f113313a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_labels = ['I-location', 'O', 'O', 'O', 'O', 'I-group', 'O', 'I-product', 'I-product', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'O', 'O', 'I-group', 'O', 'O', 'O', 'O', 'O', 'O', 'I-group', 'I-product', 'I-product', 'I-location', 'O', 'I-location', 'O', 'O', 'B-person']\n",
    "\n",
    "true_labels = ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-location', 'I-location', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n",
    "\n",
    "\n",
    "compute_reward(pred_labels, true_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fde96698-89e7-4bc2-9615-8c464289215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id                                             tokens   \n",
      "0      3014  ['Fourteen' 'chapters' 'in' '.' 'January' \"'s\"...  \\\n",
      "1       103  ['We' 'are' 'one' 'step' 'closer' 'to' 'our' '...   \n",
      "2       210  ['Man' ':' 'Hey' 'there' ',' 'havent' 'I' 'see...   \n",
      "3      1911  ['Gonna' 'be' 'on' 'national' 'television' 'mo...   \n",
      "4      2513  ['RT' '@followFDD' ':' 'The' 'case' 'for' 'dea...   \n",
      "...     ...                                                ...   \n",
      "17607  2478  ['literally' 'though' 'Saturday' 'meeting' 'th...   \n",
      "17608  3158  ['#NowPlaying' ':' 'I' 'LOVE' 'MAKONNEN' 'Feat...   \n",
      "17609   987  ['RT' '@Desbishop' ':' 'Just' 'about' 'to' 'go...   \n",
      "17610   860  ['Hurry' 'up' '!' 'Brandy' 'will' 'be' 'Leavin...   \n",
      "17611  2586  ['Also' 'having' 'a' 'hard' 'time' 'figuring' ...   \n",
      "\n",
      "                                                ner_tags   \n",
      "0                        [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]  \\\n",
      "1            [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "2      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "3      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "4                            [0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "...                                                  ...   \n",
      "17607      [0 0 0 0 0 0 0 0 0 7 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "17608                  [0 0 5 6 6 0 0 9 0 0 0 0 0 0 0 0]   \n",
      "17609                  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "17610                [0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "17611  [0 0 0 0 0 0 0 0 5 0 0 0 0 0 0 0 5 0 0 0 5 6 6...   \n",
      "\n",
      "                                               input_ids   \n",
      "0      [  101  7426  9159  1999  1012  2254  1005  10...  \\\n",
      "1      [  101  2057  2024  2028  3357  3553  2000  22...   \n",
      "2      [ 101 2158 1024 4931 2045 1010 4033 2102 1045 ...   \n",
      "3      [  101  6069  2022  2006  2120  2547  6928  19...   \n",
      "4      [  101 19387  1030  3582  2546 14141  1024  19...   \n",
      "...                                                  ...   \n",
      "17607  [  101  6719  2295  5095  3116  1996  3057  44...   \n",
      "17608  [  101  1001  2085 13068  2075  1024  1045  22...   \n",
      "17609  [  101 19387  1030  4078 18477 18471  1024  20...   \n",
      "17610  [  101  9241  2039   999 17951  2097  2022  29...   \n",
      "17611  [  101  2036  2383  1037  2524  2051 23218  20...   \n",
      "\n",
      "                                          token_type_ids   \n",
      "0      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...  \\\n",
      "1        [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "2      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "3      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "4      [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "...                                                  ...   \n",
      "17607  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "17608  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "17609        [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]   \n",
      "17610  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "17611  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0...   \n",
      "\n",
      "                                          attention_mask   \n",
      "0      [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...  \\\n",
      "1        [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   \n",
      "2      [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "3      [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "4      [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "...                                                  ...   \n",
      "17607  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   \n",
      "17608  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "17609        [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]   \n",
      "17610  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "17611  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1...   \n",
      "\n",
      "                                                  labels   \n",
      "0      [-100    0    0    0    0    0    0 -100    0 ...  \\\n",
      "1      [-100    0    0    0    0    0    0    0    0 ...   \n",
      "2      [-100    0    0    0    0    0    0 -100    0 ...   \n",
      "3      [-100    0    0    0    0    0    0    0    0 ...   \n",
      "4      [-100    0    0 -100 -100 -100    0    0    0 ...   \n",
      "...                                                  ...   \n",
      "17607  [-100    0    0    0    0    0    0    0    0 ...   \n",
      "17608  [-100    0 -100 -100 -100    0    5    6    6 ...   \n",
      "17609  [-100    0    0 -100 -100 -100    0    0    0 ...   \n",
      "17610  [-100    0    0    0    9    0    0    0    0 ...   \n",
      "17611  [-100    0    0    0    0    0    0    0    0 ...   \n",
      "\n",
      "                                             true_labels   \n",
      "0      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  \\\n",
      "1      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "3      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "...                                                  ...   \n",
      "17607  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "17608  ['O', 'O', 'B-group', 'I-group', 'I-group', 'O...   \n",
      "17609  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   \n",
      "17610  ['O', 'O', 'O', 'B-person', 'O', 'O', 'O', 'O'...   \n",
      "17611  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gr...   \n",
      "\n",
      "                                             pred_labels  reward  \n",
      "0      ['I-group', 'B-group', 'I-location', 'B-group'...   0.000  \n",
      "1      ['I-group', 'I-product', 'B-group', 'I-corpora...   0.000  \n",
      "2      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  10.000  \n",
      "3      ['B-creative-work', 'B-corporation', 'I-person...   0.800  \n",
      "4      ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...  10.000  \n",
      "...                                                  ...     ...  \n",
      "17607  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   4.000  \n",
      "17608  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', ...   4.000  \n",
      "17609  ['I-location', 'O', 'O', 'O', 'O', 'O', 'O', '...   8.125  \n",
      "17610  ['I-group', 'B-group', 'I-corporation', 'B-cre...   0.000  \n",
      "17611  ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-gr...  10.000  \n",
      "\n",
      "[17612 rows x 10 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read in the four dataframes\n",
    "df1 = pd.read_csv(\"train_df_avg_reward_0.3571.csv\")\n",
    "df2 = pd.read_csv(\"train_df_avg_reward_4.0345.csv\")\n",
    "df3 = pd.read_csv(\"train_df_avg_reward_7.4654.csv\")\n",
    "df4 = pd.read_csv(\"train_df_avg_reward_10.0.csv\")\n",
    "\n",
    "# concatenate the dataframes\n",
    "concatenated_df = pd.concat([df1, df2, df3, df4])\n",
    "\n",
    "# shuffle the concatenated dataframe\n",
    "concatenated_df = concatenated_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# view the concatenated dataframe\n",
    "print(concatenated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d513ecf5-2886-480b-a433-75a887915213",
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenated_df.to_csv(\"training_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f642e8-999a-48ec-a9b2-55061dacb833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
